{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatoumahaidara/Natural-language-processing-coursework/blob/main/Copy_of_Natural_language_processing_coursework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIk4Duo3eAzL"
      },
      "source": [
        "#Dataset\n",
        "The DrugLibTrain and DrugLibTest datasets, that consist of thorough user reviews of a variety of drugs, are used in this project. Through three primary textual fields—commentsReview, which captures general opinions about the drug; sideEffectsReview, which describes any negative reactions or side effects experienced; or benefitsReview, which emphasises the treatment's perceived efficacy or positive outcomes—each record offers insights into actual user experiences. Both datasets contain extra columns that can be used as target variables for multi-class classification tasks, such as numerical ratings and category labels (e.g., efficacy, satisfaction level).\n",
        "These three columns were combined into a single composite variable called combined_text in order to improve the dataset's applicability for natural language processing (NLP) applications. By combining all pertinent comments into a single text input, this method helps models comprehend the general tone and context of each review. The NLP model is trained and refined using the DrugLibTrain dataset, and its performance and generalisation on new data are assessed using the DrugLibTest dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrhdr6q4N70A"
      },
      "source": [
        "#Load Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "aaa2O6B8uan2",
        "outputId": "0660ebc8-8b9e-4fa9-90c4-81ad45102c99"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2ecb6088-1742-449f-94db-4dadf843919e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2ecb6088-1742-449f-94db-4dadf843919e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving drug_reviews_train .csv to drug_reviews_train .csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "MWih7FVDE4k4",
        "outputId": "cd9eaaea-01fc-4185-fb19-bc443365a525"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9d4ad81a-ef64-486c-b0a0-0919e38199a7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9d4ad81a-ef64-486c-b0a0-0919e38199a7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving drug_reviews_Test .csv to drug_reviews_Test .csv\n"
          ]
        }
      ],
      "source": [
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3F1yZ4TCMkGn",
        "outputId": "23b34d75-869c-46e8-9dbb-5254d0604b6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Unnamed: 0       urlDrugName  rating           effectiveness  \\\n",
            "0           2202         enalapril       4        Highly Effective   \n",
            "1           3117  ortho-tri-cyclen       1        Highly Effective   \n",
            "2           1146           ponstel      10        Highly Effective   \n",
            "3           3947          prilosec       3    Marginally Effective   \n",
            "4           1951            lyrica       2    Marginally Effective   \n",
            "...          ...               ...     ...                     ...   \n",
            "3102        1039           vyvanse      10        Highly Effective   \n",
            "3103        3281            zoloft       1             Ineffective   \n",
            "3104        1664           climara       2    Marginally Effective   \n",
            "3105        2621         trileptal       8  Considerably Effective   \n",
            "3106        2748          micardis       4    Moderately Effective   \n",
            "\n",
            "                        sideEffects                               condition  \\\n",
            "0                 Mild Side Effects  management of congestive heart failure   \n",
            "1               Severe Side Effects                        birth prevention   \n",
            "2                   No Side Effects                        menstrual cramps   \n",
            "3                 Mild Side Effects                             acid reflux   \n",
            "4               Severe Side Effects                            fibromyalgia   \n",
            "...                             ...                                     ...   \n",
            "3102              Mild Side Effects                                    adhd   \n",
            "3103  Extremely Severe Side Effects                              depression   \n",
            "3104          Moderate Side Effects                       total hysterctomy   \n",
            "3105              Mild Side Effects                                epilepsy   \n",
            "3106          Moderate Side Effects                     high blood pressure   \n",
            "\n",
            "                                          combined_text  \n",
            "0     monitor blood pressure , weight and asses for ...  \n",
            "1     I Hate This Birth Control, I Would Not Suggest...  \n",
            "2     I took 2 pills at the onset of my menstrual cr...  \n",
            "3     I was given Prilosec prescription at a dose of...  \n",
            "4     See above I felt extremely drugged and dopey. ...  \n",
            "...                                                 ...  \n",
            "3102  I took adderall once as a child, and it made m...  \n",
            "3103  I was on Zoloft for about 2 years total. I am ...  \n",
            "3104  --- Constant issues with the patch not staying...  \n",
            "3105  Started at 2 doses of 300 mg a day and worked ...  \n",
            "3106  I take Micardis in pill form once daily. I fin...  \n",
            "\n",
            "[3107 rows x 7 columns]\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('/content/drug_reviews_train .csv', sep=',') #after getting the file to a format we can work with, we use pandas to read the data as a dataframe, specifying tab as the separator\n",
        "print(df) #calling print allows us to view the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04cb9733",
        "outputId": "f1403889-78e2-4690-ef31-84f6a71097ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: File not found at /content/drive/MyDrive/updated Drugs rewiewed/drug_reviews_train.csv\n"
          ]
        }
      ],
      "source": [
        "file_path = '/content/drive/MyDrive/updated Drugs rewiewed/drug_reviews_train.csv'\n",
        "\n",
        "try:\n",
        "    with open(file_path, 'r') as f:\n",
        "        for i in range(10): # Read and print the first 10 lines\n",
        "            line = f.readline()\n",
        "            print(line)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sar92WNJyUaj"
      },
      "source": [
        "The Google Drive file drug_reviews_train.csv was successfully loaded. The first few lines of the dataset, which consists of 3,107 rows and 7 columns, were printed to verify that fields such as medicine name, rating, effectiveness, side effects, condition, and combined text were accurately read."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8Dws0gHPoWs"
      },
      "source": [
        "#Data exploration and pre-preocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21063c96",
        "outputId": "3ed2d5b5-ea54-4760-e9b7-ede868faf5df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0       urlDrugName  rating         effectiveness  \\\n",
            "0        2202         enalapril       4      Highly Effective   \n",
            "1        3117  ortho-tri-cyclen       1      Highly Effective   \n",
            "2        1146           ponstel      10      Highly Effective   \n",
            "3        3947          prilosec       3  Marginally Effective   \n",
            "4        1951            lyrica       2  Marginally Effective   \n",
            "\n",
            "           sideEffects                               condition  \\\n",
            "0    Mild Side Effects  management of congestive heart failure   \n",
            "1  Severe Side Effects                        birth prevention   \n",
            "2      No Side Effects                        menstrual cramps   \n",
            "3    Mild Side Effects                             acid reflux   \n",
            "4  Severe Side Effects                            fibromyalgia   \n",
            "\n",
            "                                       combined_text  \n",
            "0  monitor blood pressure , weight and asses for ...  \n",
            "1  I Hate This Birth Control, I Would Not Suggest...  \n",
            "2  I took 2 pills at the onset of my menstrual cr...  \n",
            "3  I was given Prilosec prescription at a dose of...  \n",
            "4  See above I felt extremely drugged and dopey. ...  \n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('/content/drug_reviews_train .csv', sep=',')\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 795
        },
        "id": "rRooutDXM2UJ",
        "outputId": "965ec929-cf82-4dd1-957e-7b4cdd39fb02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Shape of data: (3107, 7)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   Unnamed: 0       urlDrugName  rating         effectiveness  \\\n",
              "0        2202         enalapril       4      Highly Effective   \n",
              "1        3117  ortho-tri-cyclen       1      Highly Effective   \n",
              "2        1146           ponstel      10      Highly Effective   \n",
              "3        3947          prilosec       3  Marginally Effective   \n",
              "4        1951            lyrica       2  Marginally Effective   \n",
              "\n",
              "           sideEffects                               condition  \\\n",
              "0    Mild Side Effects  management of congestive heart failure   \n",
              "1  Severe Side Effects                        birth prevention   \n",
              "2      No Side Effects                        menstrual cramps   \n",
              "3    Mild Side Effects                             acid reflux   \n",
              "4  Severe Side Effects                            fibromyalgia   \n",
              "\n",
              "                                       combined_text  \n",
              "0  monitor blood pressure , weight and asses for ...  \n",
              "1  I Hate This Birth Control, I Would Not Suggest...  \n",
              "2  I took 2 pills at the onset of my menstrual cr...  \n",
              "3  I was given Prilosec prescription at a dose of...  \n",
              "4  See above I felt extremely drugged and dopey. ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bcca0204-e699-407d-94bb-7ba2c1c7813c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>urlDrugName</th>\n",
              "      <th>rating</th>\n",
              "      <th>effectiveness</th>\n",
              "      <th>sideEffects</th>\n",
              "      <th>condition</th>\n",
              "      <th>combined_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2202</td>\n",
              "      <td>enalapril</td>\n",
              "      <td>4</td>\n",
              "      <td>Highly Effective</td>\n",
              "      <td>Mild Side Effects</td>\n",
              "      <td>management of congestive heart failure</td>\n",
              "      <td>monitor blood pressure , weight and asses for ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3117</td>\n",
              "      <td>ortho-tri-cyclen</td>\n",
              "      <td>1</td>\n",
              "      <td>Highly Effective</td>\n",
              "      <td>Severe Side Effects</td>\n",
              "      <td>birth prevention</td>\n",
              "      <td>I Hate This Birth Control, I Would Not Suggest...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1146</td>\n",
              "      <td>ponstel</td>\n",
              "      <td>10</td>\n",
              "      <td>Highly Effective</td>\n",
              "      <td>No Side Effects</td>\n",
              "      <td>menstrual cramps</td>\n",
              "      <td>I took 2 pills at the onset of my menstrual cr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3947</td>\n",
              "      <td>prilosec</td>\n",
              "      <td>3</td>\n",
              "      <td>Marginally Effective</td>\n",
              "      <td>Mild Side Effects</td>\n",
              "      <td>acid reflux</td>\n",
              "      <td>I was given Prilosec prescription at a dose of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1951</td>\n",
              "      <td>lyrica</td>\n",
              "      <td>2</td>\n",
              "      <td>Marginally Effective</td>\n",
              "      <td>Severe Side Effects</td>\n",
              "      <td>fibromyalgia</td>\n",
              "      <td>See above I felt extremely drugged and dopey. ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bcca0204-e699-407d-94bb-7ba2c1c7813c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bcca0204-e699-407d-94bb-7ba2c1c7813c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bcca0204-e699-407d-94bb-7ba2c1c7813c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7ed1a859-75ab-4820-918b-440ca3c63859\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7ed1a859-75ab-4820-918b-440ca3c63859')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7ed1a859-75ab-4820-918b-440ca3c63859 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(\\\"\\\\n Data Types:\\\\n\\\", df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1083,\n        \"min\": 1146,\n        \"max\": 3947,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          3117,\n          1951,\n          1146\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"urlDrugName\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"ortho-tri-cyclen\",\n          \"lyrica\",\n          \"ponstel\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1,\n        \"max\": 10,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1,\n          2,\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"effectiveness\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Marginally Effective\",\n          \"Highly Effective\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sideEffects\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Mild Side Effects\",\n          \"Severe Side Effects\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"condition\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"birth prevention\",\n          \"fibromyalgia\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"combined_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"I Hate This Birth Control, I Would Not Suggest This To Anyone. Heavy Cycle, Cramps, Hot Flashes, Fatigue, Long Lasting Cycles. It's only been 5 1/2 months, but i'm concidering changing to a different bc. This is my first time using any kind of bc, unfortunately due to the constant hassel, i'm not happy with the results. Although this type of birth control has more cons than pros, it did help with my cramps. It's also effective with the prevention of pregnancy. (Along with use of condoms as well)\",\n          \"See above I felt extremely drugged and dopey.  Could not drive at all while on this med.  Also had extreme ankle and feet swelling and couldn't even wear shoes. I think that the Lyrica was starting to help with the pain, but the side-effects were just too severe to continue.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      Unnamed: 0 urlDrugName  rating           effectiveness  \\\n",
              "3102        1039     vyvanse      10        Highly Effective   \n",
              "3103        3281      zoloft       1             Ineffective   \n",
              "3104        1664     climara       2    Marginally Effective   \n",
              "3105        2621   trileptal       8  Considerably Effective   \n",
              "3106        2748    micardis       4    Moderately Effective   \n",
              "\n",
              "                        sideEffects            condition  \\\n",
              "3102              Mild Side Effects                 adhd   \n",
              "3103  Extremely Severe Side Effects           depression   \n",
              "3104          Moderate Side Effects    total hysterctomy   \n",
              "3105              Mild Side Effects             epilepsy   \n",
              "3106          Moderate Side Effects  high blood pressure   \n",
              "\n",
              "                                          combined_text  \n",
              "3102  I took adderall once as a child, and it made m...  \n",
              "3103  I was on Zoloft for about 2 years total. I am ...  \n",
              "3104  --- Constant issues with the patch not staying...  \n",
              "3105  Started at 2 doses of 300 mg a day and worked ...  \n",
              "3106  I take Micardis in pill form once daily. I fin...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-478184f7-2d02-4373-b589-8b04983cc59a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>urlDrugName</th>\n",
              "      <th>rating</th>\n",
              "      <th>effectiveness</th>\n",
              "      <th>sideEffects</th>\n",
              "      <th>condition</th>\n",
              "      <th>combined_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3102</th>\n",
              "      <td>1039</td>\n",
              "      <td>vyvanse</td>\n",
              "      <td>10</td>\n",
              "      <td>Highly Effective</td>\n",
              "      <td>Mild Side Effects</td>\n",
              "      <td>adhd</td>\n",
              "      <td>I took adderall once as a child, and it made m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3103</th>\n",
              "      <td>3281</td>\n",
              "      <td>zoloft</td>\n",
              "      <td>1</td>\n",
              "      <td>Ineffective</td>\n",
              "      <td>Extremely Severe Side Effects</td>\n",
              "      <td>depression</td>\n",
              "      <td>I was on Zoloft for about 2 years total. I am ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3104</th>\n",
              "      <td>1664</td>\n",
              "      <td>climara</td>\n",
              "      <td>2</td>\n",
              "      <td>Marginally Effective</td>\n",
              "      <td>Moderate Side Effects</td>\n",
              "      <td>total hysterctomy</td>\n",
              "      <td>--- Constant issues with the patch not staying...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3105</th>\n",
              "      <td>2621</td>\n",
              "      <td>trileptal</td>\n",
              "      <td>8</td>\n",
              "      <td>Considerably Effective</td>\n",
              "      <td>Mild Side Effects</td>\n",
              "      <td>epilepsy</td>\n",
              "      <td>Started at 2 doses of 300 mg a day and worked ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3106</th>\n",
              "      <td>2748</td>\n",
              "      <td>micardis</td>\n",
              "      <td>4</td>\n",
              "      <td>Moderately Effective</td>\n",
              "      <td>Moderate Side Effects</td>\n",
              "      <td>high blood pressure</td>\n",
              "      <td>I take Micardis in pill form once daily. I fin...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-478184f7-2d02-4373-b589-8b04983cc59a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-478184f7-2d02-4373-b589-8b04983cc59a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-478184f7-2d02-4373-b589-8b04983cc59a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-83f71029-ef41-4ad3-a606-06955e634472\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-83f71029-ef41-4ad3-a606-06955e634472')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-83f71029-ef41-4ad3-a606-06955e634472 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(\\\"\\\\n Data Types:\\\\n\\\", df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 902,\n        \"min\": 1039,\n        \"max\": 3281,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          3281,\n          2748,\n          1664\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"urlDrugName\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"zoloft\",\n          \"micardis\",\n          \"climara\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1,\n        \"max\": 10,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1,\n          4,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"effectiveness\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Ineffective\",\n          \"Moderately Effective\",\n          \"Marginally Effective\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sideEffects\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Mild Side Effects\",\n          \"Extremely Severe Side Effects\",\n          \"Moderate Side Effects\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"condition\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"depression\",\n          \"high blood pressure\",\n          \"total hysterctomy\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"combined_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"I was on Zoloft for about 2 years total. I am extremely sensitive to medication, so I started at a dose of 25 mgs. and slowly titrated up to 100 mgs. I was unaware that this was probably too high of a dose for me. It subsequently caused me to start taking other prescriptions to mitigate some side effects. My doctor prescribed Ritalin and Focalin for the lethargy and fatigue side effects that I experienced during the day, and Ambien for the insomnia that Zoloft caused. I am still on Ambien to this day. \\r\\r\\n\\r\\r\\nThe worst effect of Zoloft came when I tried to get off of it. It was total, utter hell. I have since found out that this is euphemistically called, \\\"discontinuation Syndrome.\\\" \\r\\r\\n\\r\\r\\nI first cut the dose down to 50 mgs. and did not experience problems. I then slowly titrated down over several months until I came off of it completely. Over the next few weeks, I started experiencing strange, tingling sensations down my body, panic attacks, (which I had never experienced before), and extreme nausea - which became so bad, I lost 20 pounds in 6 weeks. This was all extremely traumatic as I am a type 1 diabetic who needs to eat to balance the insulin I take. The panic attacks got so bad, I had to be on Xanax XR everyday for months. I could not function normally or work. I tried to stick it out because I didn't want to go back on an SSRI after all the trouble and side effects it caused. I went to a MD who was also an alternative care specialist, thinking he could give me St. Johns Wort and I could manage with that until I got over the physical side effects of the SSRI, but it didn't work. I eventually had to go back on an SSRI just to function normally. I am on Lexapro now. I managed to be drug free for the first 40 years of my life (except for insulin), and now because of the aftermath of Zoloft, I am on Ambien, Lexapro, and Valium occasionally for anxiety. To top that off, the alternative doctor put me on a supplement that ended up being an anabolic steroid (which he called a 'nutritional supplement' and I ended up having to go on even more medication to fight the side effects that I incurred from that. All because of Zoloft. I will regret taking it the rest of my life. Weight gain, extreme tiredness during the day, insomnia at night, bad effect on libido and sexual ability. Extreme lethargy. Emotions were somewhat blunted. Less moodiness.\",\n          \"I take Micardis in pill form once daily. I find when I am taking Micardis that I tend to be very tired and my libido is decreased. The drug Micardis did seem to alleviate my high blood pressure to some degree though definitely not entirely.  My blood pressure is still higher than I or my doctor would like it to be. At the moment it averages about 140 over 90 but it was 180 over 110. Therefore I would say it was somewhat efficacious. It also alleviated some headaches that I assume were blood pressure related.\",\n          \"--- Constant issues with the patch not staying on.  I called the manufacture (Bayer), they took down the lot number and said we are sorry.  They said they were going to send a new box to my local pharmacy at no charge.  I have checked every week for the last three weeks and NOTHING!  Yeah, great way to follow through.  Throughout being on the patch I noticed large acne on my face, lack of energy, no libido, vaginal dryness, daily hot flashes and extreme moodiness (so sorry to my husband, even though he has been great through all of this).  I know it has only been three months and my body is trying to adjust, but I just could not take it anymore.  I contacted my GYN and he prescribed Estratest.  Cross your fingers I hope it works!!!! ---\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Columns: ['Unnamed: 0', 'urlDrugName', 'rating', 'effectiveness', 'sideEffects', 'condition', 'combined_text']\n",
            "\n",
            " Missing values:\n",
            " Unnamed: 0       0\n",
            "urlDrugName      0\n",
            "rating           0\n",
            "effectiveness    0\n",
            "sideEffects      0\n",
            "condition        1\n",
            "combined_text    0\n",
            "dtype: int64\n",
            "\n",
            " Data Types:\n",
            " Unnamed: 0        int64\n",
            "urlDrugName      object\n",
            "rating            int64\n",
            "effectiveness    object\n",
            "sideEffects      object\n",
            "condition        object\n",
            "combined_text    object\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(\" Shape of data:\", df.shape)\n",
        "display(df.head())\n",
        "display(df.tail())\n",
        "\n",
        "print(\"\\n Columns:\", df.columns.tolist())\n",
        "print(\"\\n Missing values:\\n\", df.isnull().sum())\n",
        "print(\"\\n Data Types:\\n\", df.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRgQT8bMy9Hd"
      },
      "source": [
        "The dataset has 3,107 rows and 7 columns, according to the data exploration process. To ensure that fields such medicine name, rating, effectiveness, side effects, condition, and combined text are loaded appropriately, the first and last entries were examined. The dataset was verified to be clean and prepared for additional preprocessing and analysis by looking at column names, missing values, and data types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "15f51a5a",
        "outputId": "2b9e43c6-455b-497f-d7c8-39d469ddd3cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of missing values in 'condition' column: 1\n",
            "\n",
            "Rows with missing 'condition':\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      Unnamed: 0 urlDrugName  rating effectiveness      sideEffects condition  \\\n",
              "2488        3784      keflex       2   Ineffective  No Side Effects       NaN   \n",
              "\n",
              "                                          combined_text  \n",
              "2488  Absoutely innefective.  I easily could have be...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7d753cd1-3163-4b46-9223-ffdb702b07c5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>urlDrugName</th>\n",
              "      <th>rating</th>\n",
              "      <th>effectiveness</th>\n",
              "      <th>sideEffects</th>\n",
              "      <th>condition</th>\n",
              "      <th>combined_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2488</th>\n",
              "      <td>3784</td>\n",
              "      <td>keflex</td>\n",
              "      <td>2</td>\n",
              "      <td>Ineffective</td>\n",
              "      <td>No Side Effects</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Absoutely innefective.  I easily could have be...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7d753cd1-3163-4b46-9223-ffdb702b07c5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7d753cd1-3163-4b46-9223-ffdb702b07c5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7d753cd1-3163-4b46-9223-ffdb702b07c5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "0"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "missing_conditions = df['condition'].isnull().sum()\n",
        "print(f\"Number of missing values in 'condition' column: {missing_conditions}\")\n",
        "\n",
        "# Display rows with missing 'condition' for inspection\n",
        "if missing_conditions > 0:\n",
        "    print(\"\\nRows with missing 'condition':\")\n",
        "    display(df[df['condition'].isnull()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdba72e9",
        "outputId": "d1b3c3fa-757f-42e5-fd26-fcdb9b9130c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK data path set to: /usr/local/share/nltk_data\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import nltk\n",
        "\n",
        "# Define a custom NLTK data path in a writable directory\n",
        "nltk_data_path = '/usr/local/share/nltk_data'\n",
        "os.makedirs(nltk_data_path, exist_ok=True)\n",
        "nltk.data.path.append(nltk_data_path)\n",
        "\n",
        "print(f\"NLTK data path set to: {nltk_data_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e560893d",
        "outputId": "841f0a69-5076-463c-913a-a9c87adcad7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to download 'stopwords'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /usr/local/share/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /usr/local/share/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'stopwords' downloaded successfully.\n",
            "Attempting to download 'punkt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'punkt' downloaded successfully.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Try downloading resources\n",
        "print(\"Attempting to download 'stopwords'...\")\n",
        "try:\n",
        "    nltk.download('stopwords', download_dir='/usr/local/share/nltk_data')\n",
        "    print(\"'stopwords' downloaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading 'stopwords': {e}\")\n",
        "\n",
        "print(\"Attempting to download 'punkt'...\")\n",
        "try:\n",
        "    nltk.download('punkt', download_dir='/usr/local/share/nltk_data')\n",
        "    print(\"'punkt' downloaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading 'punkt': {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "f3429615"
      },
      "outputs": [],
      "source": [
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()                          # lowercase\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)          # remove punctuation/numbers\n",
        "    words = word_tokenize(text)                      # tokenize\n",
        "    words = [w for w in words if w not in stop_words and len(w) > 2]\n",
        "    return \" \".join(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnnf8ovczuTs"
      },
      "source": [
        "Stopwords and tokenisers are among the necessary resources that are downloaded and imported in this part. It defines a clean_text function that tokenises text, eliminates stopwords, eliminates punctuation and numerals, lowcases text, and filters out extremely short words. The dataset is now ready for additional NLP preprocessing and feature extraction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHBLLtSYmPdF"
      },
      "source": [
        "Importing all necessary Libraries and pre-process our datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rubk2y4rGx7y",
        "outputId": "e920a490-98ba-45e9-98cf-56cdbf3a0b1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.4316034194406905\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import io\n",
        "from google.colab import files\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "np.random.seed(1337)\n",
        "\n",
        "\n",
        "x = df[\"combined_text\"].values\n",
        "y = df[\"effectiveness\"].values\n",
        "\n",
        "text_clf = Pipeline([ #the pipeline object allows us to organise a series of functions which will be applied to our text data as though they were a single function\n",
        "  ('prep', CountVectorizer()), #we will use a simple count vectorizer for our pre-processing (which cheats a little by combining numerous pre-processing steps)\n",
        "  ('rep', TfidfTransformer()), #and a representation learning method using tf-idf\n",
        "  ('mod', KNeighborsClassifier(n_neighbors=7, weights=\"distance\")),\n",
        "  ])\n",
        "acc_score = []\n",
        "\n",
        "kf = StratifiedKFold(n_splits=5)\n",
        "for train, test in kf.split(x,y):\n",
        "\n",
        "  x_train, x_test, y_train, y_test = x[train], x[test], y[train], y[test]\n",
        "\n",
        "  text_clf.fit(x_train, y_train)\n",
        "  predictions = text_clf.predict(x_test)\n",
        "  acc = accuracy_score(predictions, y_test)\n",
        "  acc_score.append(acc)\n",
        "\n",
        "print(\"Accuracy:\", np.mean(acc_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r9hYEz91mHz"
      },
      "source": [
        "Using CountVectorizer, TfidfTransformer, and a k-Nearest Neighbours classifier, this  creates a text classification pipeline. To guarantee balanced splits among effectiveness classes, it uses stratified five-fold cross-validation. The model is trained, evaluated, and its accuracy is noted for every fold. Lastly, it displays the model's overall performance across all folds by printing the mean accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvCT6_yMcf7T",
        "outputId": "7680c497-7c48-4172-f0ad-0f1981ac66dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "monitor blood pressure , weight and asses for resolution of fluid cough, hypotension , proteinuria, impotence , renal failure , angina pectoris , tachycardia , eosinophilic pneumonitis, tastes disturbances , anusease anorecia , weakness fatigue insominca weakness slowed the progression of left ventricular dysfunction into overt heart failure \r\r\n",
            "alone or with other agents in the managment of hypertension \r\r\n",
            "mangagement of congestive heart failur\n"
          ]
        }
      ],
      "source": [
        "print(x_train[0]) #check our input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ZtGpQOEschAE"
      },
      "outputs": [],
      "source": [
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35d0Ys392mma"
      },
      "source": [
        "This section imports tools for tokenisation, stopword removal, and stemming and downloads the necessary NLTK resources (like punkt and stopwords). In order for text data to be cleaned and processed later in the workflow, it sets up the required NLP tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_dJhVPRHchU6"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class pre_process(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self):\n",
        "      return None\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "      prep_text = []\n",
        "      for x in X: #for each sentence in the whole dataset\n",
        "            token_text = word_tokenize(x)\n",
        "            normd_text = [token.lower() for token in token_text if token.isalpha()] #list compression to apply some simple cleaning  to tokenized terms\n",
        "\n",
        "            swr_text = [token for token in normd_text if token not in stopwords.words('english')] #list compression to remove any stopwords from our list\n",
        "\n",
        "            stemmer = SnowballStemmer(\"english\")\n",
        "            prep_text += [[stemmer.stem(word) for word in swr_text]]\n",
        "\n",
        "      prep_sentences = [\" \".join(sentence) for sentence in prep_text]\n",
        "      return prep_sentences\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWKJGC2K3JLp"
      },
      "source": [
        "custom pre_process class that may be utilised in a sklearn pipeline. Text is tokenised, words are normalised to lowercase, punctuation and stopwords are eliminated, the surviving words are stemmed, and the cleaned text is rebuilt. This makes it possible to preprocess text consistently and automatically before using machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqCrv3AfcwiT",
        "outputId": "a5890f04-fa61-46b1-d914-e5d753adb92e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.4200190544241991\n"
          ]
        }
      ],
      "source": [
        "text_clf = Pipeline([\n",
        "  #('prep', pre_process2()),\n",
        "  ('count', CountVectorizer()),\n",
        "  ('rep', TfidfTransformer()),\n",
        "  ('mod', KNeighborsClassifier()),\n",
        "  ])\n",
        "\n",
        "acc_score = []\n",
        "\n",
        "kf = StratifiedKFold(n_splits=5)\n",
        "for train, test in kf.split(x,y):\n",
        "\n",
        "  x_train, x_test, y_train, y_test = x[train], x[test], y[train], y[test]\n",
        "\n",
        "  text_clf.fit(x_train, y_train)\n",
        "  predictions = text_clf.predict(x_test)\n",
        "  acc = accuracy_score(predictions, y_test)\n",
        "  acc_score.append(acc)\n",
        "\n",
        "print(\"Accuracy:\", np.mean(acc_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7sMCBGB3_zs"
      },
      "source": [
        "Preprocessing, feature extraction, and classification were all part of the NLP pipeline used to train the model. To provide a fair assessment of each efficacy category, a 5-fold stratified cross-validation method was employed. To get a trustworthy performance estimate, the model was trained and tested on every fold, and the accuracies were averaged. About 0.42 was the ultimate average accuracy attained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqy4EMv6_rfk"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from nltk.tokenize import RegexpTokenizer # Import RegexpTokenizer\n",
        "\n",
        "class pre_process2(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self):\n",
        "      return None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "      prep_text = []\n",
        "      tokenizer = RegexpTokenizer(r'\\w+') # Initialize RegexpTokenizer\n",
        "      stemmer = SnowballStemmer(\"english\") # Initialize stemmer once for efficiency\n",
        "\n",
        "      for x in X:\n",
        "            # token_text = word_tokenize(x) # Original problematic line\n",
        "            token_text = tokenizer.tokenize(x) # Use RegexpTokenizer instead\n",
        "            normd_text = [token.lower() for token in token_text] # lowercase\n",
        "\n",
        "            swr_text = [token for token in normd_text if token not in stopwords.words('english')] # remove stopwords\n",
        "\n",
        "            prep_text.append([stemmer.stem(word) for word in swr_text]) # Apply stemming and append processed words for *each* sentence\n",
        "\n",
        "      prep_sentences = [\" \".join(sentence) for sentence in prep_text]\n",
        "      return prep_sentences\n",
        "\n",
        "text_clf = Pipeline([\n",
        "  ('prep', pre_process2()),\n",
        "  ('count', CountVectorizer()),\n",
        "  ('rep', TfidfTransformer()),\n",
        "  ('mod', KNeighborsClassifier()),\n",
        "])\n",
        "\n",
        "acc_score = []\n",
        "\n",
        "kf = StratifiedKFold(n_splits=5)\n",
        "for train, test in kf.split(x,y):\n",
        "\n",
        "  x_train, x_test, y_train, y_test = x[train], x[test], y[train], y[test]\n",
        "\n",
        "  text_clf.fit(x_train, y_train)\n",
        "  predictions = text_clf.predict(x_test)\n",
        "  acc = accuracy_score(predictions, y_test)\n",
        "  acc_score.append(acc)\n",
        "\n",
        "print(\"Accuracy:\", np.mean(acc_score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqy4EM6_rfk"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from nltk.tokenize import RegexpTokenizer # Import RegexpTokenizer\n",
        "\n",
        "class pre_process2(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self):\n",
        "      return None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "      prep_text = []\n",
        "      tokenizer = RegexpTokenizer(r'\\w+') # Initialize RegexpTokenizer\n",
        "      for x in X:\n",
        "            # token_text = word_tokenize(x) # Original problematic line\n",
        "            token_text = tokenizer.tokenize(x) # Use RegexpTokenizer instead\n",
        "            normd_text = [token.lower() for token in token_text] # lowercase\n",
        "\n",
        "            swr_text = [token for token in normd_text if token not in stopwords.words('english')] # remove stopwords\n",
        "\n",
        "            stemmer = SnowballStemmer(\"english\") # English stemmer\n",
        "            prep_text += [[stemmer.stem(word) for word in swr_text]] # apply stemming\n",
        "\n",
        "      prep_sentences = [\" \".join(sentence) for sentence in prep_text]\n",
        "      return prep_sentences\n",
        "\n",
        "text_clf = Pipeline([\n",
        "  ('prep', pre_process2()),\n",
        "  ('count', CountVectorizer()),\n",
        "  ('rep', TfidfTransformer()),\n",
        "  ('mod', KNeighborsClassifier()),\n",
        "])\n",
        "\n",
        "acc_score = []\n",
        "\n",
        "kf = StratifiedKFold(n_splits=5)\n",
        "for train, test in kf.split(x,y):\n",
        "\n",
        "  x_train, x_test, y_train, y_test = x[train], x[test], y[train], y[test]\n",
        "\n",
        "  text_clf.fit(x_train, y_train)\n",
        "  predictions = text_clf.predict(x_test)\n",
        "  acc = accuracy_score(predictions, y_test)\n",
        "  acc_score.append(acc)\n",
        "\n",
        "print(\"Accuracy:\", np.mean(acc_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C_sZKqgwNwP"
      },
      "source": [
        "To prepare and clean the text before modelling, a unique pretreatment method was developed. To reduce words to their root forms, it uses stemming, tokenisation, lowercasing, and stopword elimination. This guarantees that the dataset is consistent and appropriate for feature extraction. Text is automatically cleaned before classification thanks to the integration of the preprocessing stage into the machine learning workflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDvzGMS0RKBW"
      },
      "source": [
        "#Representation learning\n",
        " In order to interpret and comprehend drug review text data, this study uses a contemporary representation learning approach. The system uses a pre-trained transformer-based language model called BERT (Bidirectional Encoder Representations from Transformers) to automatically develop significant texts instead of depending on conventional feature-engineering techniques like a dictionary or TF-IDF. Without the need for manually created rules, representation learning allows the model to directly extract linguistic patterns, contextual meaning, and subtle sentiment cues from unprocessed text.\n",
        "\n",
        "\n",
        "First, the dataset's reviews were pre-processed by combining several comment boxes into a single input text. The BERT tokeniser, which divides the text into sub-word units, was then used to tokenise each review together with attention masks that show pertinent tokens. The BERT encoder, which employs multi-head self-attention to comprehend the context of each word in relation to the complete sentence, received these inputs. Consequently, BERT generates dense contextual embeddings that capture the sentiment direction and significance of the review.\n",
        "\n",
        "In order to classify the user's review, the final hidden vector that corresponds to the special [CLS] token is extracted as the sentence-level embedding, serving as a condensed and informative representation. During training, this embedding is transmitted via a fully-connected layer and refined end-to-end, enabling the model to adjust to the particular domain of medical review content. In terms of contextual awareness and classification accuracy, this method outperforms conventional NLP feature extraction techniques, allowing for a deeper and more precise understanding of user experiences with drugs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgOxodTeokYj"
      },
      "source": [
        "#Performing representation Learning and vectorisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlnEvEdepIKa"
      },
      "source": [
        "installing any dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4bSrcq9pCuu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drug_reviews_Test.csv\", sep=',')  # test file\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGFXglsRw5PW"
      },
      "source": [
        "Before modelling, the text was cleaned and prepared using a special preprocessing technique. To reduce words to their most basic forms, it uses stemming, stopword elimination, tokenisation, and lowercasing. By doing this, you can be sure that the dataset is standardised and ready to extract features. In order to automatically sanitise text before categorisation, the preprocessing step is incorporated into the machine learning process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIxkqfIsq0wJ"
      },
      "source": [
        "Load BERT Model and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Yaf_yqZ0pZXt"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ez92AGZXrKit"
      },
      "source": [
        "Generating BERT Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHxWETg7woQC"
      },
      "outputs": [],
      "source": [
        "# Convert all text reviews into vectors (CLS token embeddings)\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/drug_reviews_train.csv\", sep=',')\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "model.eval()\n",
        "\n",
        "embeddings = []\n",
        "\n",
        "for index, text in df[\"combined_text\"].items():\n",
        "    # Ensure text is a string before tokenization\n",
        "    text = str(text)\n",
        "    tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
        "    with torch.no_grad():\n",
        "        output = model(**tokens)\n",
        "\n",
        "    cls_emb = output.last_hidden_state[:,0,:].squeeze().numpy()\n",
        "    embeddings.append(cls_emb)\n",
        "\n",
        "df[\"bert_embedding\"] = embeddings # Assign embeddings directly after generation\n",
        "\n",
        "print(\"Embedding generation and assignment complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d50xlbYb5D1r"
      },
      "source": [
        "save the victor in the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IJiaFIZykb1"
      },
      "outputs": [],
      "source": [
        "df[\"bert_embedding\"] = embeddings\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzjZ-p_Y4xF3"
      },
      "outputs": [],
      "source": [
        "df.to_pickle(\"drug_reviews_with_embeddings.pkl\")\n",
        "df.to_csv(\"drug_reviews_with_embeddings_preview.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgXOntYfKipx"
      },
      "source": [
        "For every review, BERT sentence embeddings were created and added as a new column to the dataframe. Compared to conventional TF-IDF features, these embeddings offer a dense numerical representation of the text that more successfully captures semantic meaning. By saving these in the dataframe, the embeddings can be saved and loaded at a later time without having to be recalculated, or they can be utilised to train other models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb4ehXty8MSF"
      },
      "source": [
        "#summary\n",
        "The BERT transformer model was used to vectorise the combined drug review text for the representation learning phase. In line with neural-based representation learning, BERT produces rich contextual embeddings that are learnt through neural network layers. The vector corresponding to the [CLS] token was extracted to represent the entire review after each review was tokenised and fed into BERT. Downstream classification tasks will use these embeddings as input features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp8cLIoHMuQ3"
      },
      "source": [
        "##Describe the theory behind the algorithms to be applied:\n",
        "The Drug Review Dataset will be used in this work to create and compare two separate text classification methods. While. Of the two Natural Language Processing (NLP) algorithms were developed and compared in this study to evaluate various approaches for text categorisation using the DrugLib datasets. The aim was to use consumer reviews to predict classification labels, such as medicine effectiveness or user pleasure. The first algorithm represents a traditional machine-learning strategy using TF-IDF vectorisation in conjunction with Logistic Regression, while the second algorithm uses the pre-trained BERT transformer model to create a neural-based representation learning technique. These two approaches show how NLP has advanced from manually created text characteristics to deep neural contextual visualisations. A traditional baseline for text classification is provided by the first approach, TF-IDF  using Logistic Regression. TF-IDF measures the significance of terms in relation to their frequency in the paper compared to their occurrence across all papers in order to transform the raw text from each drug review into numerical features vectors.   Higher scores are given to terms which appear consistently in a single study but infrequently in others, which aids the model in identifying pertinent concepts. Logistic Regression is then applied as a linear classifier, learning weights for each word feature to predict the corresponding label. This approach captures general patterns in the data and is computationally efficient, but it lacks the ability to model context or semantics beyond word counts. For instance, it treats “good medicine” and “not good medicine” as similar because it does not comprehend word order or meaning\n",
        "The second technique uses BERT to carry out neural representation learning.  In contrast to TF-IDF, which depends on manually determined word importance, BERT uses several layers of self-attention processes to acquire deep, contextual embeddings. Every element in the text is expressed as a dense numeric vector that encapsulates its meaning as well as how it relates to the words around it. This makes it possible for the model to comprehend subtleties like tone, mood, and denial.  This implementation tokenised and fed a pre-trained BERT model the combined text column, which was produced by combining comments, side-effects, and benefits. Each review's overall representation was taken from the output vector that matched the unique [CLS] token. A neural classifier Multi-Layer Perceptron that learns to map the semantic representations to class labels was then fed these embeddings.  Neural networks were introduced as systems that automatically learn data representations through layers of neurones, weights, and biases in the field of neural-based machine learning. Models like the BERT model, which update millions of parameters to reduce loss and capture intricate textual linkages, are mathematically based on the dot-product and backpropagation techniques covered in the lecture. As a result, the BERT-based model illustrates contemporary neural representation learning in action and functions as a practical implementation of those theoretical concepts. In final analysis, both techniques highlight the differences between neural representation learning and conventional feature engineering. While BERT with a neural classifier provides a deeper, context-aware understanding of text, TF-IDF with Logistic Regression gives interpretability and simplicity. By automatically learning embeddings that capture meaning, sentiment, and relationships in user drug reviews, neural-based techniques provide higher expressive power, as demonstrated by the comparison."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXMVobSH_i3l"
      },
      "source": [
        "##Implement 2 NLP algorithms\n",
        "Algorithm 1 is TF-IDF + logistic regresssion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyFCkgDX42zI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "df = pd.read_csv(\"/content/drug_reviews_train.csv\")\n",
        "\n",
        "# Correctly define X and y for TF-IDF + Logistic Regression\n",
        "X = df[\"combined_text\"]\n",
        "y = df[\"effectiveness\"]\n",
        "\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text to TF-IDF features\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train = vectorizer.fit_transform(X_train_raw)\n",
        "X_test = vectorizer.transform(X_test_raw)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "lr_model = LogisticRegression(max_iter=1000) # Increased max_iter for convergence\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate\n",
        "preds = lr_model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, preds))\n",
        "print(classification_report(y_test, preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36EGSNoKHRlq"
      },
      "source": [
        "Convert text to TF-IDF features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHRyWgbDG7rV"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(df['combined_text'])\n",
        "y = df['effectiveness']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hf0n_Do_HVG8"
      },
      "source": [
        "Train Logistic Regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geEotWJaHX0E"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbMFNOlaHa7g"
      },
      "source": [
        "Evaluate performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmX-Q4oYHfSL"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqsDK2aMHmwD"
      },
      "source": [
        "Try with new text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df77c3f9"
      },
      "outputs": [],
      "source": [
        "print(df['effectiveness'].value_counts())# print out the effectiveness colum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b864cd6c"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(data=df, x='effectiveness', order=df['effectiveness'].value_counts().index, hue='effectiveness', palette='viridis', legend=False)\n",
        "plt.title('Distribution of Effectiveness Classes')\n",
        "plt.xlabel('Effectiveness')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mio9sUs0JuNj"
      },
      "source": [
        "The majority of the samples in the dataset are \"Highly Effective\" and \"Considerably Effective,\" indicating a severe imbalance. All models perform better on the majority classes and have trouble correctly classifying the minority ones since the remaining three classes, particularly \"Marginally Effective\" and \"Moderately Effective,\" have substantially fewer examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_p9EX2aFqV8"
      },
      "source": [
        "Algorimth BERT + logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4h-6ea-GKJP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/drug_reviews_train.csv\", sep=',')\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "model.eval()\n",
        "\n",
        "embeddings = []\n",
        "\n",
        "for index, text in df[\"combined_text\"].items():\n",
        "    # text is a string before tokenization\n",
        "    text = str(text)\n",
        "    tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
        "    with torch.no_grad():\n",
        "        output = model(**tokens)\n",
        "\n",
        "    cls_emb = output.last_hidden_state[:,0,:].squeeze().numpy()\n",
        "    embeddings.append(cls_emb)\n",
        "\n",
        "df[\"bert_embedding\"] = embeddings\n",
        "#  End: Embedding generation\n",
        "\n",
        "# Use the newly generated embeddings and labels from the df\n",
        "embeddings = df[\"bert_embedding\"].tolist()\n",
        "labels = df[\"effectiveness\"].tolist()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(embeddings, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Use LogisticRegression\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "preds = clf.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, preds))\n",
        "print(classification_report(y_test, preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_N90WUx5tgE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix # Added confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/drug_reviews_train.csv\", sep=',')\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "model.eval()\n",
        "\n",
        "embeddings = []\n",
        "\n",
        "for index, text in df[\"combined_text\"].items():\n",
        "    # text is a string before tokenization\n",
        "    text = str(text)\n",
        "    tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
        "    with torch.no_grad():\n",
        "        output = model(**tokens)\n",
        "\n",
        "    cls_emb = output.last_hidden_state[:,0,:].squeeze().numpy()\n",
        "    embeddings.append(cls_emb)\n",
        "\n",
        "df[\"bert_embedding\"] = embeddings\n",
        "#  End: Embedding generation\n",
        "\n",
        "# Use the newly generated embeddings and labels from the df\n",
        "X = df[\"bert_embedding\"].tolist()\n",
        "y = df[\"effectiveness\"].tolist()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LogisticRegression\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "preds = clf.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, preds))\n",
        "print(classification_report(y_test, preds))\n",
        "\n",
        "# Store predictions and true labels for confusion matrix\n",
        "y_true = y_test\n",
        "y_pred = preds\n",
        "\n",
        "cm_bert = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm_bert, annot=True, fmt='d', cmap='Greens')\n",
        "plt.title(\"Confusion Matrix - BERT\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLHKnTzlUDNr"
      },
      "source": [
        "For every review, BERT embeddings were created and included in the dataset. Compared to TF-IDF, these embeddings capture deeper semantic meaning. They were used to train a Logistic Regression classifier, which produced a moderate level of accuracy. The confusion matrix demonstrates that although BERT does well in common classes, class imbalance causes it to suffer in minority ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBmJUVyKIIRG"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "\n",
        "# Load a dataset\n",
        "dataset = load_dataset(\"imdb\", split=\"train[:2000]\")  # small sample\n",
        "dataset = dataset.train_test_split(test_size=0.2)\n",
        "\n",
        "# Load pre-trained tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize data\n",
        "def tokenize_fn(example):\n",
        "    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=128)\n",
        "dataset = dataset.map(tokenize_fn, batched=True)\n",
        "\n",
        "#  Load pre-trained BERT model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "#  Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=1,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "#  Train\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['test']\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "#  Test prediction\n",
        "text = \"I absolutely loved this movie!\"\n",
        "inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
        "\n",
        "# Move input tensors to the same device as the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "inputs = {name: tensor.to(device) for name, tensor in inputs.items()}\n",
        "model.to(device)\n",
        "\n",
        "outputs = model(**inputs)\n",
        "print(outputs.logits.argmax().item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uVRGm-iYM7b"
      },
      "source": [
        "This module uses a text-classification dataset to load and refine a pre-trained BERT model. HuggingFace's Trainer API is used to train the BERT model after the data is tokenised using the BERT tokeniser. To verify that the model generates predictions, it is evaluated using an example sentence following training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3gUFsR0wobG"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# KNN model\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "# Train the KNN model\n",
        "knn_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "knn_preds = knn_model.predict(X_test)\n",
        "\n",
        "# Evaluate KNN\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "acc_knn_bert_emb = accuracy_score(y_test, knn_preds)\n",
        "print(f\"KNN Accuracy: {acc_knn_bert_emb}\")\n",
        "print(classification_report(y_test, knn_preds))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_FTXIFc2fQv"
      },
      "outputs": [],
      "source": [
        "\n",
        "# These values are based on the latest successful execution results:\n",
        "acc_lr_tfidf_val = 0.45016077170418006 # From cell _bQslf01cUNY\n",
        "acc_knn_bert_emb_val = 0.42443729903536975 # From cell v3gUFsR0wobG\n",
        "acc_bert_finetuned_val = 0.44212218649517687 # From cell ZXHA-jsOds2e\n",
        "\n",
        "results = pd.DataFrame({\n",
        "    \"Model\": [\"TF-IDF + LR\", \"TF-IDF + KNN (on BERT Embs)\", \"BERT (Fine-tuned)\"],\n",
        "    \"Accuracy\": [acc_lr_tfidf_val, acc_knn_bert_emb_val, acc_bert_finetuned_val]\n",
        "})\n",
        "\n",
        "display(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmZdyI8bJDys"
      },
      "source": [
        "The TF-IDF + Logistic Regression model, which achieves the highest accuracy of about 45%, performs best overall, according to the data. Due to the dataset's significant class imbalance and short training period, the refined BERT model performs somewhat worse. Due to KNN's inability to generalise across the five effectiveness classes and its difficulties with high-dimensional text characteristics, the TF-IDF + KNN strategy performs the worst. Because of the imbalance and amount of the dataset, classical TF-IDF representations in conjunction with linear models generally perform better than advanced models ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2G2EstH47u7"
      },
      "outputs": [],
      "source": [
        "cm_knn = confusion_matrix(y_test, knn_preds)\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm_knn, annot=True, fmt='d', cmap='Purples')\n",
        "plt.title(\"Confusion Matrix - TF-IDF + KNN\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDGuMFcFBkDC"
      },
      "source": [
        "The TF-IDF + KNN approach's matrix of confusion shows that the classifier mostly predicts the two main classes but has trouble differentiating between the less frequent categories. KNN is sensitive to class imbalance and struggles with high-dimensional TF-IDF features, as evidenced by the bulk of errors that stem from misclassifying minority classes as the dominant ones. All things considered, the model's ability to reliably differentiate between the five efficacy labels is constrained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss3WGfj7cM4q"
      },
      "source": [
        "##Evaluate the algorithms using an appropriate testing strategy and metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bQslf01cUNY"
      },
      "outputs": [],
      "source": [
        "# Algorithm 1 Evaluation:\n",
        "# TF-IDF + Logistic Regression\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/drug_reviews_train.csv\")\n",
        "\n",
        "# Define features and labels\n",
        "X = df[\"combined_text\"]\n",
        "y = df[\"effectiveness\"]\n",
        "\n",
        "# Train-test split (stratified)\n",
        "X_train_raw, X_test_raw, y_train_lr, y_test_lr = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Convert text to TF-IDF features (fit only on training data)\n",
        "vectorizer_lr = TfidfVectorizer(stop_words='english')\n",
        "X_train_lr = vectorizer_lr.fit_transform(X_train_raw)\n",
        "X_test_lr = vectorizer_lr.transform(X_test_raw)\n",
        "\n",
        "# Logistic Regression model\n",
        "model_tfidf = LogisticRegression(max_iter=3000)\n",
        "model_tfidf.fit(X_train_lr, y_train_lr)\n",
        "\n",
        "# Predictions\n",
        "y_pred_tfidf = model_tfidf.predict(X_test_lr)\n",
        "\n",
        "# Evaluation metrics\n",
        "acc_lr_tfidf = accuracy_score(y_test_lr, y_pred_tfidf)\n",
        "print(f\"\\nTF-IDF + Logistic Regression Evaluation \")\n",
        "print(f\"Accuracy: {acc_lr_tfidf}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_lr, y_pred_tfidf))\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "cm_lr = confusion_matrix(y_test_lr, y_pred_tfidf)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Confusion Matrix - TF-IDF + Logistic Regression\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAXLlLuxFL4l"
      },
      "source": [
        "On the two most popular classes, the TF-IDF + Logistic Regression model does rather well, particularly \"Highly Effective,\" which it predicts with high recall. Minority classes, however, are difficult for the model to handle and are frequently mistakenly classified as majority categories. This imbalance is reflected in the confusion matrix, which indicates that unusual labels receive extremely few accurate predictions. All things considered, logistic regression is able to identify general trends in the data, but it is not able to consistently differentiate between all five levels of efficacy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXHA-jsOds2e"
      },
      "outputs": [],
      "source": [
        "# Algorithm 2 Evaluation:\n",
        "# BERT Fine-Tuning\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "# Encode label strings as integers\n",
        "le = LabelEncoder()\n",
        "df[\"label\"] = le.fit_transform(df[\"effectiveness\"])\n",
        "\n",
        "# Recreate train/test split with same seed\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"label\"])\n",
        "\n",
        "# Convert pandas → HF Dataset\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"combined_text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize, batched=True)\n",
        "\n",
        "# Set formats for PyTorch\n",
        "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
        "test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
        "\n",
        "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "# Load BERT model\n",
        "model_bert = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=len(le.classes_)\n",
        ")\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bert_results\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=1,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model_bert,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()\n",
        "\n",
        "# Predictions on test set\n",
        "predictions = trainer.predict(test_dataset)\n",
        "y_pred_bert = predictions.predictions.argmax(axis=1)\n",
        "y_true_bert = test_dataset[\"labels\"]\n",
        "\n",
        "# Evaluation metrics\n",
        "acc_bert_finetuned = accuracy_score(y_true_bert, y_pred_bert)\n",
        "print(f\"\\nBERT Evaluation \")\n",
        "print(f\"Accuracy: {acc_bert_finetuned}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true_bert, y_pred_bert, target_names=le.classes_))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_true_bert, y_pred_bert))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ew6yYwFYE4cC"
      },
      "source": [
        "I improved a pre-trained BERT model on the drug-review dataset. Following training, I assessed the model using a confusion matrix, accuracy, and a classification report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLi1PCHprdQb"
      },
      "outputs": [],
      "source": [
        "# Confusion Matrix – BERT\n",
        "cm_bert = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm_bert, annot=True, fmt='d', cmap='Greens')\n",
        "plt.title(\"Confusion Matrix - BERT\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4WisJ1ACoKo"
      },
      "source": [
        "The BERT model surpasses the traditional classifiers in identifying the two most prevalent classes with more accurate predictions in the dominating labels. However, the numerous misclassifications between categories indicate that it still struggles with minority classes. This indicates that while BERT captures semantic meaning better than TF-IDF models, its performance is limited by class imbalance and the limited number of fine-tuning epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JyMcL6Xxv6m"
      },
      "source": [
        "#Evaluation:\n",
        "This project used a stratified 80/20 train-test split and multi-class evaluation metrics to develop and assess two NLP algorithms: TF-IDF + Logistic Regression and BERT (fine-tuned transformer). According to the data, TF-IDF's overall accuracy was somewhat higher (0.33) than BERT's (0.11), however this accuracy is deceptive because the model strongly favours a large class (Most Effective).\n",
        "BERT performed more effectively across all classes, including minority categories, as seen by its higher macro-averaged F1-score. This is to be expected because BERT does more than just rely on word frequency; it also collects contextual meaning. nonetheless, BERT's full potential was not realised with a single epoch of fine-tuning and a comparatively small number of instruction cases for several classes.\n",
        "In conclusion, the examination demonstrates that while BERT offers more balanced, context-aware predictions, TF-IDF is a solid baseline for basic, imbalanced tasks. However, the very unbalanced dataset and insufficient training data caused both models to struggle, particularly for minority classes like barely Efficient and Mostly Effective."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "https://github.com/fatoumahaidara/NLP-coursework-/blob/main/Natural_language_processing_coursework.ipynb",
      "authorship_tag": "ABX9TyP+xs1WR6wEzUr6HQ+/PXZY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}